{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Chatbot using Google NarrativeQA Reading Comprehension Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook ingests, cleans, and saves the questions in one CSV and the answers in another CSV.\n",
    "INGEST: The original dataset provided by Google was of the following schema: \n",
    "* qaps.csv: document_id, set, question, answer1, answer2, question_tokenized, answer1_tokenized, answer2_tokenized.\n",
    "\n",
    "EXPORT: I cleaned and parsed the dataset to export the following:\n",
    "* questions_list.csv: document_id and question\n",
    "* answers_list.csv: answer1 and answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joelhaas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import pickle\n",
    "import string\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean each question and answer\n",
    "\n",
    "def cleaning_document(document):\n",
    "    regex = r\"\\([^)]*\\)|\\[[^)]*\\]\"   #removes anything in () or [] and trailing spaces\n",
    "    \n",
    "    document = document.lower()\n",
    "        \n",
    "    document = re.sub(' +', ' ', document)    \n",
    "    document = document.replace(\" â€¢\", \"\")\n",
    "    document = document.replace(\" .\", \".\")\n",
    "    document = document.replace(\"-\", \" \")\n",
    "    document = document.replace(\"#\",\"\")\n",
    "    document = document.replace(\"/\",\" \")\n",
    "    document = document.replace(\"\\\\\",\" \")\n",
    "    document = document.replace(\"TM\",\" \")\n",
    "    document = document.replace(\"\\n\", \" \")\n",
    "    document = document.replace(\"\\t\", \" \")\n",
    "    document = document.replace(\"test,\\\"\", \" \")\n",
    "    document = document.replace(\"train,\\\"\", \" \")\n",
    "    document = document.replace(\"  \", \" \")\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten lists of list \n",
    "\n",
    "def nested_flatten(inputList):\n",
    "    summary = []\n",
    "    \n",
    "    for item in inputList:\n",
    "        if isinstance(item, list):\n",
    "            summary += nested_flatten(item)\n",
    "        else:\n",
    "            summary += [item]\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    num_files = 0\n",
    "\n",
    "    answers_list = []\n",
    "    questions_list = []\n",
    "    \n",
    "    # Ingest qas.csv file\n",
    "    with open('qas.csv', newline='') as csvfile:\n",
    "        spamreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in spamreader:\n",
    "            num_files += 1\n",
    "            \n",
    "            try:\n",
    "                row = str(row[0])\n",
    "                row = row.split(',') # split each row by a comma\n",
    "\n",
    "                answerID = row[0]  # equivalent to doc_id\n",
    "                question = row[2]  # question being asked\n",
    "\n",
    "                answer1 = row[3]   # annotated answer by participant\n",
    "                answer2 = row[4]   # annotated answer by paarticipant\n",
    "                \n",
    "                # prep question for CSV export\n",
    "                updatedQuestion = answerID + ', ' + question + ' '\n",
    "                updatedQuestion = cleaning_document(updatedQuestion) # clean sentence\n",
    "                questions_list.append(updatedQuestion)\n",
    "                \n",
    "                # prep answers for CSV export\n",
    "                answers = answer1 + ', ' + answer2 + ' '\n",
    "                answers = cleaning_document(answers) # clean sentence\n",
    "                answers_list.append(answers)\n",
    "            \n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    \n",
    "    # Export questions and answers to CSVs\n",
    "    df1 = pd.DataFrame(data={\"questions\": questions_list})\n",
    "    df1.to_csv(\"questions_list.csv\", sep=',', encoding='utf-8',index=False)      \n",
    "\n",
    "    df2 = pd.DataFrame(data={\"answers\": answers_list})\n",
    "    df2.to_csv(\"answers_list.csv\", sep=',', encoding='utf-8',index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
